sampler:
  type: GENEky
  ion_scale_bounds: [0.4, 0.5]
  electron_scale_bounds: [0.5,0.55]
  num_ion_scale_samples: 5
  num_electron_scale_samples: 5
  sub_sampler:
    type: Grid #This must take in at least, parameters, bounds, num_samples

executor:
  type: DaskExecutorSimulation
  base_run_dir: /scratch/project_462000451/enchanted_test_out/gene_scan_runner
  runner_return_headder: 'GENEky, mixing_length_phi, mixing_length_A, electron_heat_diffusivity_ratio, chi_i/chi_e, D_e/chi_e, growthrate, frequency'  
  runner:
    type: GeneScanRunner
    executable_path: /scratch/project_462000451/gene_enchanted/enchanted_executable/gene_lumi_csc
    scanscript_path: /scratch/project_462000451/gene_enchanted/enchanted_executable/scanscript
    # These should be the same as the worker_args
    time: '12:00:00'
    account: 'project_462000451'
    n_jobs: 2
    ssh_command_to_login_node: 'ssh -i ~/.ssh/lumi-key uan03'
    gene_dir: /scratch/project_462000451/gene_enchanted/
    #You can specify a base_parameters_file_path
    #Although this means each worker must read from this file
    # It is more optimal to use GENEparser.print_default_nml_string(base_parameters_file_path) to get a string
    # Then paste this string directly into the GENEparser --> __init__ --> self.base_params_nml_string varaible
    #base_parameters_file_path: /project/project_462000451/DEEPlasma/parameters_base_jet_97781
    
  worker_args: # to be specified for the SLURMCluster in DASK
    name: GENEcluster
    cores: 1
    memory: "2GB" # Memory on a lumi cpu node 256GB
    walltime: "12:00:00" #Max time limit of entire cluster
    job_extra_directives:
      - "--nodes 1"
      - "--partition small"
      - "--account project_462000451"
      - "--ntasks 2"
      # - '-o /scratch/project_462000451/gene_out/enchanted_out/gene_test2/%x.%j.out'
      # - '-e /scratch/project_462000451/gene_out/enchanted_out/gene_test2/%x.%j.err'
    interface: "nmn0"
    # The job script prologue is ran on every worker at start up
    job_script_prologue: # this is possibly dependent on which code you want
      - 'export PATH="/project/project_462000451/enchanted_container_lumi3/bin:$PATH"'
      - unset SLURM_MEM_PER_CPU #For some reason when running from sbatch this is being set and causing an error as is conflicts with mem_per_node
      - export LD_LIBRARY_PATH=/opt/cray/pe/papi/7.1.0.1/lib64:/opt/cray/libfabric/1.15.2.0/lib64
      # Change this for your own personal enchanted surrogates clone
      - 'cd /users/danieljordan/enchanted-surrogates2/'
      # This command gives the worker access to the python within our container. 
      - 'export PYTHONPATH=$PYTHONPATH:/users/danieljordan/enchanted-surrogates2/src' # NB: to use the enchanted-surrogate library
    # These are appended to the sbatch that requestes the resources for each worker.
  n_jobs: 2