{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow for modeling\n",
    "\n",
    "Using outputs from data_sprint_anna. Duplicating the same logic for folder names etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "import scipy.special\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = 1 # 0 for random sampling, 1 for Static Sparce Grid\n",
    "num_samples = 15 # if using SSG, it will find the level to get at least this amount of samples\n",
    "\n",
    "#set the bounds, order t_eped, n_eped, d_n_ped (=d_T_ped), n_esep\n",
    "bounds = [[1.0, 1.8], [3.0, 4.4], [0.05, 0.1], [0.5, 2.8]]\n",
    "\n",
    "#set output directory (can be empty or non-empty for analysis)\n",
    "if sampling == 0:\n",
    "    base_run_dir = '/scratch/project_462000451/daniel/sprint_out/helena/full_test'\n",
    "else:\n",
    "    base_run_dir = '/scratch/project_462000451/daniel/sprint_out/helena/19_2_SSG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following HELENA output directories were found in the folder\n",
      "['1.4-3.7-0.07500000000000001-0.07500000000000001-0.5000000000000003', '1.4-4.4-0.07500000000000001-0.07500000000000001-1.65', '1.8-3.7-0.07500000000000001-0.07500000000000001-2.8', '1.8-3.7-0.07500000000000001-0.07500000000000001-1.65', '1.4-3.7-0.07500000000000001-0.07500000000000001-2.8', '1.4-3.7-0.1-0.1-2.8', '1.4-3.7-0.1-0.1-1.65', '1.8-4.4-0.07500000000000001-0.07500000000000001-1.65', '1.4-3.7-0.05000000000000001-0.05000000000000001-1.65', '1.4-3.7-0.07500000000000001-0.07500000000000001-1.65', '1.4-4.4-0.1-0.1-1.65', '1.0-3.7-0.07500000000000001-0.07500000000000001-1.65', '1.4-4.4-0.07500000000000001-0.07500000000000001-2.8', '1.4-3.0-0.07500000000000001-0.07500000000000001-1.65', '1.8-3.7-0.1-0.1-1.65']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_folder_name = '-'.join([str(b) for b in np.array(bounds).flatten()])\n",
    "hel_output_names = []\n",
    "if os.path.exists(os.path.join(base_run_dir, run_folder_name)):\n",
    "    hel_output_names = [name for name in os.listdir(os.path.join(base_run_dir, run_folder_name)) \n",
    "                    if os.path.isdir(os.path.join(base_run_dir, run_folder_name, name))]\n",
    "    print(\"The following HELENA output directories were found in the folder\")\n",
    "    print(hel_output_names)\n",
    "else:\n",
    "    print(os.path.join(base_run_dir, run_folder_name))\n",
    "    print(\"Given directory does not excist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find DREAM. Please add the $DREAMPATH/py to your PYTHONPATH environment variable before running. No module named 'DREAM'\n",
      "['/opt/cray/pe/python/3.11.7/lib/python311.zip', '/opt/cray/pe/python/3.11.7/lib/python3.11', '/opt/cray/pe/python/3.11.7/lib/python3.11/lib-dynload', '', '/scratch/project_462000451/daniel/daniel_sprint/lib/python3.11/site-packages', '/project/project_462000451/enchanted-surrogates_11feb2025/submodules', '/project/project_462000451/enchanted-surrogates_11feb2025/submodules/IFS_scripts', '/project/project_462000451/enchanted-surrogates_11feb2025/src']\n",
      "ky =  0.048351505\n",
      "x [ 3.57494130e+00  4.87839320e-01  4.87839320e-01  4.83515050e-02\n",
      "  4.32779218e+00  2.39417495e+00  8.10000000e-01  2.26091065e-03\n",
      "  1.76872368e-02  1.58952518e+00  1.36367522e+00  1.07569167e+00\n",
      "  7.98689771e-01  5.73225305e-01  4.05307216e-01  2.95409431e-01\n",
      "  2.36276507e-01  2.32767491e-01  3.03119475e-01  4.69908534e-01\n",
      "  7.69185916e-01  1.20831272e+00  1.80906419e+00  2.56913871e+00\n",
      "  3.63841460e+00  4.85035111e+00  5.97834803e+00  6.36365000e+00\n",
      "  5.81165154e+00  4.53793567e+00  3.22842117e+00  2.25843186e+00\n",
      "  1.63386614e+00  1.23913861e+00  9.68476288e-01  7.96514837e-01\n",
      "  6.92717090e-01  6.42431315e-01  6.46498651e-01  7.07776585e-01\n",
      "  8.28313113e-01  1.01428909e+00  1.25000512e+00  1.46525318e+00\n",
      "  1.60643709e+00 -1.54202229e+01 -1.27608560e+01 -9.73381988e+00\n",
      " -6.96043333e+00 -4.75886014e+00 -3.11415526e+00 -1.96530527e+00\n",
      " -1.14308154e+00 -6.07308201e-01 -2.31273443e-01 -2.50058684e-02\n",
      "  6.06302945e-02  4.83013388e-02 -9.13539690e-03 -6.38808086e-02\n",
      " -9.01551998e-02 -9.13777049e-02 -6.74243919e-02  0.00000000e+00\n",
      "  4.88755014e-02  8.21896543e-02  9.35882407e-02  1.46909185e-01\n",
      "  2.10996440e-01  2.92209702e-01  4.08798033e-01  5.71572231e-01\n",
      "  7.77058419e-01  1.07365697e+00  1.50516194e+00  2.11148644e+00\n",
      "  2.94101600e+00  4.07448101e+00  5.49419575e+00  6.88579851e+00\n",
      "  8.06236886e+00 -1.96970706e+00 -2.23539619e+00 -2.23753004e+00\n",
      " -2.01388132e+00 -1.66865805e+00 -1.26066454e+00 -8.43357793e-01\n",
      " -3.82411708e-01  9.19931921e-02  6.98792218e-01  1.39794057e+00\n",
      "  2.22766972e+00  3.06685324e+00  3.84617105e+00  4.40622109e+00\n",
      "  4.74999938e+00  4.27308987e+00  2.64012389e+00  0.00000000e+00\n",
      " -2.59758601e+00 -4.00391456e+00 -4.19041411e+00 -3.71680035e+00\n",
      " -3.13940140e+00 -2.61082408e+00 -2.09103306e+00 -1.64159066e+00\n",
      " -1.27912955e+00 -9.66536150e-01 -7.21211308e-01 -5.75859902e-01\n",
      " -5.54639956e-01 -6.74812359e-01 -9.51866292e-01 -1.35522832e+00\n",
      " -1.73767974e+00  1.50934303e+02  1.20905641e+02  8.98593808e+01\n",
      "  6.28976249e+01  4.24233910e+01  2.77953115e+01  1.80730005e+01\n",
      "  1.14355723e+01  7.25937828e+00  4.29704405e+00  2.50625990e+00\n",
      "  1.43852543e+00  8.50535277e-01  5.23048518e-01  3.39702448e-01\n",
      "  2.21680326e-01  1.54343795e-01  1.18050380e-01  1.07933720e-01\n",
      "  1.20856102e-01  1.63697480e-01  2.47387622e-01  3.88888805e-01\n",
      "  5.99123162e-01  8.92877093e-01  1.32431973e+00  1.93844341e+00\n",
      "  2.78608488e+00  4.03753014e+00  5.91893719e+00  8.67875242e+00\n",
      "  1.26249373e+01  1.82677754e+01  2.57771549e+01  3.38050448e+01\n",
      "  4.18070861e+01  1.98627197e+01  2.18465093e+01  2.14678079e+01\n",
      "  1.92433824e+01  1.62776991e+01  1.32146445e+01  1.05979155e+01\n",
      "  8.27357453e+00  6.46218287e+00  4.72584315e+00  3.36108027e+00\n",
      "  2.27469497e+00  1.43897984e+00  8.32929471e-01  4.64012142e-01\n",
      "  2.65558080e-01  1.91032097e-01  1.80754561e-01  1.94307006e-01\n",
      "  1.94572471e-01  2.16638991e-01  3.06944435e-01  4.02984164e-01\n",
      "  5.27627047e-01  6.62240655e-01  8.01462446e-01  9.13623774e-01\n",
      "  1.00232117e+00  1.02697425e+00  9.40428035e-01  6.50149400e-01\n",
      "  1.62512926e-02 -1.13508006e+00 -2.95612864e+00 -5.37912968e+00\n",
      " -7.88704535e+00  2.86965856e+00  4.24653388e+00  5.49756862e+00\n",
      "  6.36454370e+00  6.88119440e+00  7.14832360e+00  7.39345809e+00\n",
      "  7.61660963e+00  7.96379808e+00  8.33605991e+00  8.88478472e+00\n",
      "  9.54016619e+00  9.84205870e+00  9.58253885e+00  8.54609752e+00\n",
      "  6.88629819e+00  4.26278991e+00  1.55842992e+00  3.64206092e-01\n",
      "  1.56451145e+00  4.06333104e+00  6.20488576e+00  7.22882821e+00\n",
      "  7.57063809e+00  7.49877109e+00  6.99264121e+00  6.26060474e+00\n",
      "  5.47859268e+00  4.57948117e+00  3.65824273e+00  2.83594205e+00\n",
      "  2.18743125e+00  1.76410202e+00  1.65789735e+00  1.93717567e+00\n",
      "  2.40264944e+00  1.45965697e+00  1.42708882e+00  1.38337931e+00\n",
      "  1.33718269e+00  1.29283098e+00  1.25206909e+00  1.21511734e+00\n",
      "  1.18123743e+00  1.14931557e+00  1.11760017e+00  1.08493679e+00\n",
      "  1.05015018e+00  1.01260983e+00  9.72701178e-01  9.32023856e-01\n",
      "  8.93556177e-01  8.60391389e-01  8.37376914e-01  8.28765599e-01\n",
      "  8.36650730e-01  8.57960161e-01  8.88763260e-01  9.25568152e-01\n",
      "  9.66628421e-01  1.01045136e+00  1.05614794e+00  1.10331500e+00\n",
      "  1.15158583e+00  1.20045707e+00  1.24942885e+00  1.29778388e+00\n",
      "  1.34455366e+00  1.38831072e+00  1.42667290e+00  1.45558524e+00\n",
      "  1.46923526e+00  4.98259956e-01  2.67063238e-01  1.17058271e-01\n",
      "  3.54711073e-02  1.01020111e-02  2.57364614e-02  7.54710056e-02\n",
      "  1.41778537e-01  2.16787962e-01  2.86414457e-01  3.44616523e-01\n",
      "  3.77401306e-01  3.77822831e-01  3.39585829e-01  2.64688505e-01\n",
      "  1.53119361e-01  3.71916653e-02 -5.87021397e-02 -1.00066676e-01\n",
      " -6.62090992e-02  2.18193936e-02  1.20768589e-01  2.29514466e-01\n",
      "  3.33724897e-01  4.26167705e-01  5.07688067e-01  5.87906755e-01\n",
      "  6.59852301e-01  7.21446092e-01  7.70226178e-01  8.05929804e-01\n",
      "  8.32961927e-01  8.42191895e-01  8.23201343e-01  7.90833205e-01\n",
      "  6.76985678e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -1.15886568e-01 -2.33128382e-01 -2.65809981e-01\n",
      " -2.62379982e-01 -2.44081431e-01 -2.22658693e-01 -2.01302798e-01\n",
      " -1.86809293e-01 -1.78169040e-01 -1.81843650e-01 -1.92568669e-01\n",
      " -2.06985599e-01 -2.22393906e-01 -2.34034980e-01 -2.35520305e-01\n",
      " -2.10378852e-01 -1.67895034e-01 -9.74543323e-02  2.22239320e-05\n",
      "  8.35485390e-02  1.53502963e-01  1.92642564e-01  2.24254112e-01\n",
      "  2.45515560e-01  2.57708640e-01  2.65210445e-01  2.73631666e-01\n",
      "  2.79481300e-01  2.82364443e-01  2.80314075e-01  2.72691955e-01\n",
      "  2.61854939e-01  2.38887388e-01  1.92501113e-01  1.39656690e-01\n",
      "  2.59425305e-02  1.03676636e+01  9.81893915e+00  9.46707212e+00\n",
      "  9.19413744e+00  8.94360479e+00  8.69215855e+00  8.43738032e+00\n",
      "  8.19024495e+00  7.96799515e+00  7.79780114e+00  7.69998608e+00\n",
      "  7.69884619e+00  7.81497727e+00  8.06150129e+00  8.43554096e+00\n",
      "  8.91429024e+00  9.43829732e+00  9.87632091e+00  1.00529805e+01\n",
      "  9.88319733e+00  9.47325711e+00  8.98733651e+00  8.53670611e+00\n",
      "  8.15396232e+00  7.84995435e+00  7.62142099e+00  7.46129126e+00\n",
      "  7.36281605e+00  7.32289043e+00  7.34003154e+00  7.41642445e+00\n",
      "  7.55809136e+00  7.77778764e+00  8.10035018e+00  8.57252854e+00\n",
      "  9.27811425e+00  2.10725192e+00  2.15378265e+00  2.21978067e+00\n",
      "  2.29442394e+00  2.37141309e+00  2.44729001e+00  2.52081760e+00\n",
      "  2.59262368e+00  2.66460273e+00  2.74084197e+00  2.82487883e+00\n",
      "  2.92127033e+00  3.03385072e+00  3.16441175e+00  3.31053839e+00\n",
      "  3.46478970e+00  3.61210456e+00  3.72449088e+00  3.76770473e+00\n",
      "  3.72578725e+00  3.61878805e+00  3.47895648e+00  3.33033042e+00\n",
      "  3.18250705e+00  3.04063236e+00  2.90654400e+00  2.78074938e+00\n",
      "  2.66329879e+00  2.55446058e+00  2.45436981e+00  2.36338729e+00\n",
      "  2.28206329e+00  2.21145931e+00  2.15362490e+00  2.11230531e+00\n",
      "  2.09362765e+00 -1.35961601e+01 -1.21901434e+01 -1.09147283e+01\n",
      " -9.75625033e+00 -8.70229797e+00 -7.74157945e+00 -6.86380192e+00\n",
      " -6.05956113e+00 -5.32024070e+00 -4.63791974e+00 -4.00528809e+00\n",
      " -3.41556791e+00 -2.86244111e+00 -2.33998166e+00 -1.84259209e+00\n",
      " -1.36494349e+00 -9.01918504e-01 -4.48556402e-01  0.00000000e+00\n",
      "  4.48556402e-01  9.01918504e-01  1.36494349e+00  1.84259209e+00\n",
      "  2.33998166e+00  2.86244111e+00  3.41556791e+00  4.00528809e+00\n",
      "  4.63791974e+00  5.32024070e+00  6.05956113e+00  6.86380192e+00\n",
      "  7.74157945e+00  8.70229797e+00  9.75625033e+00  1.09147283e+01\n",
      "  1.21901434e+01 -2.95175402e-01 -5.85014296e-01 -8.24319410e-01\n",
      " -1.01342311e+00 -1.15715761e+00 -1.26201503e+00 -1.33408993e+00\n",
      " -1.37805405e+00 -1.39826829e+00 -1.39468266e+00 -1.37090113e+00\n",
      " -1.32714039e+00 -1.26165350e+00 -1.17127340e+00 -1.04857132e+00\n",
      " -8.80542845e-01 -6.53724019e-01 -3.62463445e-01 -2.85021654e-02\n",
      "  3.04305326e-01  5.89281948e-01  8.07544397e-01  9.62044514e-01\n",
      "  1.06589613e+00  1.13021794e+00  1.16323656e+00  1.16913820e+00\n",
      "  1.15110623e+00  1.11010305e+00  1.04582338e+00  9.56452507e-01\n",
      "  8.38828659e-01  6.88627024e-01  5.01154553e-01  2.73137162e-01\n",
      "  5.58103067e-03 -1.25556593e+00 -1.13819105e+00 -9.82073195e-01\n",
      " -8.10432570e-01 -6.42791818e-01 -4.83251897e-01 -3.39836898e-01\n",
      " -1.98688769e-01 -7.11597609e-02  7.95163624e-02  2.34525739e-01\n",
      "  4.08717586e-01  5.89318087e-01  8.10383148e-01  1.09307236e+00\n",
      "  1.49905516e+00  1.95453307e+00  2.36471884e+00  2.52262760e+00\n",
      "  2.33826300e+00  1.89802888e+00  1.41216249e+00  9.69994305e-01\n",
      "  6.24143512e-01  3.62954225e-01  1.40627698e-01 -4.80935108e-02\n",
      " -2.02577512e-01 -3.50839357e-01 -4.99230336e-01 -6.44788543e-01\n",
      " -7.89238616e-01 -9.40395980e-01 -1.08818533e+00 -1.19933645e+00\n",
      " -1.26720748e+00 -1.14365056e-01 -2.61145097e-01 -3.33502488e-01\n",
      " -3.76681333e-01 -4.00055076e-01 -4.14457215e-01 -4.24170166e-01\n",
      " -4.43620685e-01 -4.77183743e-01 -5.44790426e-01 -6.44132210e-01\n",
      " -7.75973768e-01 -9.27909741e-01 -1.07347349e+00 -1.17232366e+00\n",
      " -1.17951896e+00 -1.01496944e+00 -6.21654181e-01  0.00000000e+00\n",
      "  5.86670376e-01  9.67178370e-01  1.11093892e+00  1.14782802e+00\n",
      "  1.11548827e+00  1.05233209e+00  9.74009912e-01  8.91180003e-01\n",
      "  8.07266941e-01  7.20654791e-01  6.30291783e-01  5.40392829e-01\n",
      "  4.53227500e-01  3.60476898e-01  2.56631498e-01  1.63845224e-01\n",
      "  2.48389775e-02]\n",
      "y [0.00445503 0.01772683 0.02524657 0.04056112]\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/project/project_462000451/enchanted-surrogates_11feb2025/submodules')\n",
    "sys.path.append('/project/project_462000451/enchanted-surrogates_11feb2025/submodules/IFS_scripts')\n",
    "sys.path.append('/project/project_462000451/enchanted-surrogates_11feb2025/src')\n",
    "from parsers.GENEparser import GENEparser\n",
    "gp = GENEparser()\n",
    "scanfiles_dir = '/scratch/project_462000451/gene_out/gene_auto_97781/jet_97781_ion-scale_90/ex-jet_97781_ion-scale_90_batch-0/scanfiles0000/'\n",
    "print(sys.path)\n",
    "x = gp.get_model_inputs(scanfiles_dir, suffix='0001')\n",
    "y = gp.get_model_outputs(scanfiles_dir, suffix='0001')\n",
    "\n",
    "print('x',x)\n",
    "print('y',y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get inputs and outputs from GENE runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/project_462000451/daniel/sprint_out/gene/19_2_SSG/1.0-1.8-3.0-4.4-0.05-0.1-0.5-2.8/1.4-3.7-0.07500000000000001-0.07500000000000001-0.5000000000000003/scanfiles0003\n",
      "Run unsuccessfull for 1.4-3.7-0.07500000000000001-0.07500000000000001-0.5000000000000003\n",
      "/scratch/project_462000451/daniel/sprint_out/gene/19_2_SSG/1.0-1.8-3.0-4.4-0.05-0.1-0.5-2.8/1.4-4.4-0.07500000000000001-0.07500000000000001-1.65/scanfiles0003\n",
      "ky =  0.047635959\n",
      "ky =  0.047663208\n",
      "Run unsuccessfull for 1.4-4.4-0.07500000000000001-0.07500000000000001-1.65\n",
      "/scratch/project_462000451/daniel/sprint_out/gene/19_2_SSG/1.0-1.8-3.0-4.4-0.05-0.1-0.5-2.8/1.8-3.7-0.07500000000000001-0.07500000000000001-2.8/scanfiles0003\n",
      "ky =  0.046436764\n",
      "ky =  0.048207549\n",
      "Run unsuccessfull for 1.8-3.7-0.07500000000000001-0.07500000000000001-2.8\n",
      "/scratch/project_462000451/daniel/sprint_out/gene/19_2_SSG/1.0-1.8-3.0-4.4-0.05-0.1-0.5-2.8/1.8-3.7-0.07500000000000001-0.07500000000000001-1.65/scanfiles0003\n",
      "ky =  0.047138244\n",
      "ky =  0.048658589\n",
      "Run unsuccessfull for 1.8-3.7-0.07500000000000001-0.07500000000000001-1.65\n",
      "/scratch/project_462000451/daniel/sprint_out/gene/19_2_SSG/1.0-1.8-3.0-4.4-0.05-0.1-0.5-2.8/1.4-3.7-0.07500000000000001-0.07500000000000001-2.8/scanfiles0003\n",
      "ky =  0.052572066\n",
      "ky =  0.051563073\n",
      "Run unsuccessfull for 1.4-3.7-0.07500000000000001-0.07500000000000001-2.8\n",
      "/scratch/project_462000451/daniel/sprint_out/gene/19_2_SSG/1.0-1.8-3.0-4.4-0.05-0.1-0.5-2.8/1.4-3.7-0.1-0.1-2.8/scanfiles0003\n",
      "ky =  0.051092183\n",
      "ky =  0.051087572\n",
      "Run unsuccessfull for 1.4-3.7-0.1-0.1-2.8\n",
      "/scratch/project_462000451/daniel/sprint_out/gene/19_2_SSG/1.0-1.8-3.0-4.4-0.05-0.1-0.5-2.8/1.4-3.7-0.1-0.1-1.65/scanfiles0003\n",
      "ky =  0.051421265\n",
      "ky =  0.051329698\n",
      "Run unsuccessfull for 1.4-3.7-0.1-0.1-1.65\n",
      "/scratch/project_462000451/daniel/sprint_out/gene/19_2_SSG/1.0-1.8-3.0-4.4-0.05-0.1-0.5-2.8/1.8-4.4-0.07500000000000001-0.07500000000000001-1.65/scanfiles0003\n",
      "ky =  0.048895677\n",
      "Run unsuccessfull for 1.8-4.4-0.07500000000000001-0.07500000000000001-1.65\n",
      "/scratch/project_462000451/daniel/sprint_out/gene/19_2_SSG/1.0-1.8-3.0-4.4-0.05-0.1-0.5-2.8/1.4-3.7-0.05000000000000001-0.05000000000000001-1.65/scanfiles0003\n",
      "ky =  0.051662601\n",
      "ky =  0.052482373\n",
      "Run unsuccessfull for 1.4-3.7-0.05000000000000001-0.05000000000000001-1.65\n",
      "/scratch/project_462000451/daniel/sprint_out/gene/19_2_SSG/1.0-1.8-3.0-4.4-0.05-0.1-0.5-2.8/1.4-3.7-0.07500000000000001-0.07500000000000001-1.65/scanfiles0003\n",
      "ky =  0.053109913\n",
      "ky =  0.051868816\n",
      "Run unsuccessfull for 1.4-3.7-0.07500000000000001-0.07500000000000001-1.65\n",
      "/scratch/project_462000451/daniel/sprint_out/gene/19_2_SSG/1.0-1.8-3.0-4.4-0.05-0.1-0.5-2.8/1.4-4.4-0.1-0.1-1.65/scanfiles0003\n",
      "ky =  0.052569338\n",
      "ky =  0.047876981\n",
      "Run unsuccessfull for 1.4-4.4-0.1-0.1-1.65\n",
      "/scratch/project_462000451/daniel/sprint_out/gene/19_2_SSG/1.0-1.8-3.0-4.4-0.05-0.1-0.5-2.8/1.0-3.7-0.07500000000000001-0.07500000000000001-1.65/scanfiles0046\n",
      "ky =  0.048562731\n",
      "ky =  0.051361161\n",
      "ky =  0.049216999\n",
      "ky =  0.15108405\n",
      "ky =  0.14980339\n",
      "ky =  0.15024136\n",
      "ky =  0.25900123\n",
      "ky =  0.2610859\n",
      "ky =  0.25903684\n",
      "ky =  0.36691841\n",
      "ky =  0.36808832\n",
      "ky =  0.36524194\n",
      "ky =  0.46943973\n",
      "ky =  0.47081064\n",
      "ky =  0.46885667\n",
      "ky =  0.57735691\n",
      "ky =  0.57781306\n",
      "ky =  0.57765214\n",
      "ky =  0.67987823\n",
      "ky =  0.68053539\n",
      "ky =  0.68126688\n",
      "ky =  0.78779541\n",
      "ky =  0.78753781\n",
      "ky =  0.78747198\n",
      "ky =  0.89031674\n",
      "ky =  0.89026013\n",
      "ky =  0.89108671\n",
      "ky =  0.99823392\n",
      "ky =  1.0015426\n",
      "ky =  0.99988219\n",
      "/scratch/project_462000451/daniel/sprint_out/gene/19_2_SSG/1.0-1.8-3.0-4.4-0.05-0.1-0.5-2.8/1.4-4.4-0.07500000000000001-0.07500000000000001-2.8/scanfiles0003\n",
      "ky =  0.047120028\n",
      "ky =  0.052579383\n",
      "Run unsuccessfull for 1.4-4.4-0.07500000000000001-0.07500000000000001-2.8\n",
      "/scratch/project_462000451/daniel/sprint_out/gene/19_2_SSG/1.0-1.8-3.0-4.4-0.05-0.1-0.5-2.8/1.4-3.0-0.07500000000000001-0.07500000000000001-1.65/scanfiles0003\n",
      "ky =  0.051814903\n",
      "ky =  0.050856257\n",
      "ky =  0.049318698\n",
      "ky =  0.14896785\n",
      "ky =  0.14748315\n",
      "ky =  0.1508572\n",
      "ky =  0.25907452\n",
      "ky =  0.25936691\n",
      "ky =  0.26109899\n",
      "ky =  0.36918119\n",
      "ky =  0.36616505\n",
      "ky =  0.36553859\n",
      "ky =  0.47281099\n",
      "ky =  0.46787756\n",
      "ky =  0.46997818\n",
      "ky =  0.5764408\n",
      "ky =  0.5746757\n",
      "ky =  0.57731888\n",
      "ky =  0.68007061\n",
      "ky =  0.68147384\n",
      "ky =  0.67885738\n",
      "ky =  0.79017728\n",
      "ky =  0.78827198\n",
      "ky =  0.78909917\n",
      "ky =  0.88733022\n",
      "ky =  0.8899845\n",
      "ky =  0.89063767\n",
      "ky =  0.99743689\n",
      "ky =  1.0018683\n",
      "ky =  1.0008795\n",
      "/scratch/project_462000451/daniel/sprint_out/gene/19_2_SSG/1.0-1.8-3.0-4.4-0.05-0.1-0.5-2.8/1.8-3.7-0.1-0.1-1.65/scanfiles0003\n",
      "ky =  0.047071229\n",
      "ky =  0.049581561\n",
      "Run unsuccessfull for 1.8-3.7-0.1-0.1-1.65\n"
     ]
    }
   ],
   "source": [
    "gene_base_dir = '/scratch/project_462000451/daniel/sprint_out/gene/19_2_SSG'\n",
    "#outputs = gp.get_model_outputs('/scratch/project_462000451/daniel/sprint_out/gene/full_test/1.0-1.8-3.0-4.4-0.05-0.1-0.5-2.8/1.6907219221235361-4.066386570204081-0.08957719078481746-0.08957719078481746-1.6580178585501517/scanfiles0001', '0001')\n",
    "#inputs = gp.get_model_inputs('/scratch/project_462000451/daniel/sprint_out/gene/full_test/1.0-1.8-3.0-4.4-0.05-0.1-0.5-2.8/1.6907219221235361-4.066386570204081-0.08957719078481746-0.08957719078481746-1.6580178585501517/scanfiles0001', '0001')\n",
    "#print(inputs)\n",
    "#print(outputs)\n",
    "\n",
    "outputs_d = []\n",
    "inputs_d = []\n",
    "\n",
    "for name_path in hel_output_names:\n",
    "    scan_sufxs = gp.get_all_suffixes(os.path.join(gene_base_dir, run_folder_name, name_path))\n",
    "    last_scan_sufx = scan_sufxs[-1]\n",
    "    scan_file_path = os.path.join(gene_base_dir, run_folder_name, name_path, \"scanfiles\"+last_scan_sufx)\n",
    "    print(scan_file_path)\n",
    "    #problemd = os.path.join('/project/project_462000451/gene/', name_path)\n",
    "    sufxs = gp.get_all_suffixes(scan_file_path)\n",
    "    #print(sufxs)\n",
    "    for sufx in sufxs:\n",
    "        #print(sufx)\n",
    "        try:\n",
    "            out = gp.get_model_outputs(scan_file_path, sufx)\n",
    "            inp = gp.get_model_inputs(scan_file_path, sufx)\n",
    "            if len(sufxs) > 4:\n",
    "                outputs_d.append(out)\n",
    "                inputs_d.append(inp)\n",
    "                \n",
    "        #print(inputs)\n",
    "        #print(outputs)\n",
    "        except:\n",
    "            print(\"Run unsuccessfull for\", name_path)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.199388\n",
       "1    0.203108\n",
       "2    0.011586\n",
       "3    0.274674\n",
       "4    0.076528\n",
       "5    0.003258\n",
       "Name: 0, dtype: float128"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_o = pd.DataFrame(outputs_d)# + pd.DataFrame(outputs)\n",
    "df_o.head(30)\n",
    "df = pd.DataFrame(inputs_d)\n",
    "#df.head(5)\n",
    "df_o = df_o[0]\n",
    "df_o.head(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with the inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 585)\n",
      "(30,)\n",
      "(30, 585)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "inputs_train = np.array(inputs_d[0:30])\n",
    "outputs_train = np.array(df_o[0:30]).astype('float64')#np.array(outputs_d[0:30]).astype('float64')\n",
    "print(inputs_train.shape)\n",
    "print(outputs_train.shape)\n",
    "inputs_test = np.array(inputs_d[30:])\n",
    "outputs_test = np.array(df_o[30:]).astype('float64')#np.array(outputs_d[30:]).astype('float64')\n",
    "print(inputs_test.shape)\n",
    "print(outputs_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/project_462000451/daniel/daniel_sprint/lib/python3.11/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.15258632600307465\n",
      "Epoch 2, Loss: 5.591246128082275\n",
      "Epoch 3, Loss: 0.5585892796516418\n",
      "Epoch 4, Loss: 0.5839418768882751\n",
      "Epoch 5, Loss: 1.096593976020813\n",
      "Epoch 6, Loss: 0.4645538926124573\n",
      "Epoch 7, Loss: 0.031042547896504402\n",
      "Epoch 8, Loss: 0.2070847749710083\n",
      "Epoch 9, Loss: 0.37990739941596985\n",
      "Epoch 10, Loss: 0.18888260424137115\n",
      "Epoch 11, Loss: 0.026254253461956978\n",
      "Epoch 12, Loss: 0.05854225903749466\n",
      "Epoch 13, Loss: 0.14056791365146637\n",
      "Epoch 14, Loss: 0.12010139971971512\n",
      "Epoch 15, Loss: 0.04458463191986084\n",
      "Epoch 16, Loss: 0.00905848853290081\n",
      "Epoch 17, Loss: 0.048469021916389465\n",
      "Epoch 18, Loss: 0.08398664742708206\n",
      "Epoch 19, Loss: 0.05139349028468132\n",
      "Epoch 20, Loss: 0.013234885409474373\n",
      "Epoch 21, Loss: 0.013334348797798157\n",
      "Epoch 22, Loss: 0.03946506977081299\n",
      "Epoch 23, Loss: 0.04524888098239899\n",
      "Epoch 24, Loss: 0.025082409381866455\n",
      "Epoch 25, Loss: 0.008491131477057934\n",
      "Epoch 26, Loss: 0.01381591148674488\n",
      "Epoch 27, Loss: 0.027578217908740044\n",
      "Epoch 28, Loss: 0.02862010896205902\n",
      "Epoch 29, Loss: 0.01645672135055065\n",
      "Epoch 30, Loss: 0.007620459888130426\n",
      "Epoch 31, Loss: 0.011618677526712418\n",
      "Epoch 32, Loss: 0.019881553947925568\n",
      "Epoch 33, Loss: 0.01982870325446129\n",
      "Epoch 34, Loss: 0.012288268655538559\n",
      "Epoch 35, Loss: 0.007596025243401527\n",
      "Epoch 36, Loss: 0.010467784479260445\n",
      "Epoch 37, Loss: 0.014666236937046051\n",
      "Epoch 38, Loss: 0.011654835194349289\n",
      "Epoch 39, Loss: 0.0076789879240095615\n",
      "Epoch 40, Loss: 0.009853438474237919\n",
      "Epoch 41, Loss: 0.011674978770315647\n",
      "Epoch 42, Loss: 0.008955750614404678\n",
      "Epoch 43, Loss: 0.007427156437188387\n",
      "Epoch 44, Loss: 0.00912264920771122\n",
      "Epoch 45, Loss: 0.010057879611849785\n",
      "Epoch 46, Loss: 0.008552666753530502\n",
      "Epoch 47, Loss: 0.007395647000521421\n",
      "Epoch 48, Loss: 0.008337057195603848\n",
      "Epoch 49, Loss: 0.009117266163229942\n",
      "Epoch 50, Loss: 0.008169932290911674\n",
      "Epoch 51, Loss: 0.007364885415881872\n",
      "Epoch 52, Loss: 0.007931304164230824\n",
      "Epoch 53, Loss: 0.008490233682096004\n",
      "Epoch 54, Loss: 0.007978909648954868\n",
      "Epoch 55, Loss: 0.007372381631284952\n",
      "Epoch 56, Loss: 0.007653150241822004\n",
      "Epoch 57, Loss: 0.008090389892458916\n",
      "Epoch 58, Loss: 0.007802858017385006\n",
      "Epoch 59, Loss: 0.007370055187493563\n",
      "Epoch 60, Loss: 0.007525494787842035\n",
      "Epoch 61, Loss: 0.00782778300344944\n",
      "Epoch 62, Loss: 0.007666887249797583\n",
      "Epoch 63, Loss: 0.007368935272097588\n",
      "Epoch 64, Loss: 0.007448811549693346\n",
      "Epoch 65, Loss: 0.007657818496227264\n",
      "Epoch 66, Loss: 0.007562798447906971\n",
      "Epoch 67, Loss: 0.007362300530076027\n",
      "Epoch 68, Loss: 0.007412409409880638\n",
      "Epoch 69, Loss: 0.007551131770014763\n",
      "Epoch 70, Loss: 0.007488848175853491\n",
      "Epoch 71, Loss: 0.007356205023825169\n",
      "Epoch 72, Loss: 0.007392071187496185\n",
      "Epoch 73, Loss: 0.007483942434191704\n",
      "Epoch 74, Loss: 0.007435352075845003\n",
      "Epoch 75, Loss: 0.007349338848143816\n",
      "Epoch 76, Loss: 0.007381944451481104\n",
      "Epoch 77, Loss: 0.007439279928803444\n",
      "Epoch 78, Loss: 0.007399172056466341\n",
      "Epoch 79, Loss: 0.007346300408244133\n",
      "Epoch 80, Loss: 0.007375624962151051\n",
      "Epoch 81, Loss: 0.007408656645566225\n",
      "Epoch 82, Loss: 0.007374181877821684\n",
      "Epoch 83, Loss: 0.00734528386965394\n",
      "Epoch 84, Loss: 0.007371703162789345\n",
      "Epoch 85, Loss: 0.007387015037238598\n",
      "Epoch 86, Loss: 0.007358859293162823\n",
      "Epoch 87, Loss: 0.007346268743276596\n",
      "Epoch 88, Loss: 0.0073676030151546\n",
      "Epoch 89, Loss: 0.007370960433036089\n",
      "Epoch 90, Loss: 0.00734991068020463\n",
      "Epoch 91, Loss: 0.007348411250859499\n",
      "Epoch 92, Loss: 0.007363432552665472\n",
      "Epoch 93, Loss: 0.007359395734965801\n",
      "Epoch 94, Loss: 0.007345865480601788\n",
      "Epoch 95, Loss: 0.007350247818976641\n",
      "Epoch 96, Loss: 0.00735867815092206\n",
      "Epoch 97, Loss: 0.007351599633693695\n",
      "Epoch 98, Loss: 0.007345089688897133\n",
      "Epoch 99, Loss: 0.00735126668587327\n",
      "Epoch 100, Loss: 0.00735382828861475\n",
      "Epoch 101, Loss: 0.00734706362709403\n",
      "Epoch 102, Loss: 0.007345919962972403\n",
      "Epoch 103, Loss: 0.007350960746407509\n",
      "Epoch 104, Loss: 0.007349650375545025\n",
      "Epoch 105, Loss: 0.007345233112573624\n",
      "Epoch 106, Loss: 0.007347041741013527\n",
      "Epoch 107, Loss: 0.007349501829594374\n",
      "Epoch 108, Loss: 0.007346714846789837\n",
      "Epoch 109, Loss: 0.007345179095864296\n",
      "Epoch 110, Loss: 0.007347566075623035\n",
      "Epoch 111, Loss: 0.007347592152655125\n",
      "Epoch 112, Loss: 0.007345277816057205\n",
      "Epoch 113, Loss: 0.007345790974795818\n",
      "Epoch 114, Loss: 0.007347206585109234\n",
      "Epoch 115, Loss: 0.0073459940031170845\n",
      "Epoch 116, Loss: 0.0073450771160423756\n",
      "Epoch 117, Loss: 0.0073462179861962795\n",
      "Epoch 118, Loss: 0.007346305530518293\n",
      "Epoch 119, Loss: 0.007345162332057953\n",
      "Epoch 120, Loss: 0.0073454175144433975\n",
      "Epoch 121, Loss: 0.007346099708229303\n",
      "Epoch 122, Loss: 0.007345464080572128\n",
      "Epoch 123, Loss: 0.007345068268477917\n",
      "Epoch 124, Loss: 0.007345657330006361\n",
      "Epoch 125, Loss: 0.0073456126265227795\n",
      "Epoch 126, Loss: 0.0073450664058327675\n",
      "Epoch 127, Loss: 0.007345282938331366\n",
      "Epoch 128, Loss: 0.007345551159232855\n",
      "Epoch 129, Loss: 0.007345177233219147\n",
      "Epoch 130, Loss: 0.007345084100961685\n",
      "Epoch 131, Loss: 0.007345383055508137\n",
      "Epoch 132, Loss: 0.0073452601209282875\n",
      "Epoch 133, Loss: 0.007345035206526518\n",
      "Epoch 134, Loss: 0.007345213554799557\n",
      "Epoch 135, Loss: 0.007345265243202448\n",
      "Epoch 136, Loss: 0.0073450603522360325\n",
      "Epoch 137, Loss: 0.007345100399106741\n",
      "Epoch 138, Loss: 0.007345218677073717\n",
      "Epoch 139, Loss: 0.007345095742493868\n",
      "Epoch 140, Loss: 0.007345046382397413\n",
      "Epoch 141, Loss: 0.007345155812799931\n",
      "Epoch 142, Loss: 0.0073451148346066475\n",
      "Epoch 143, Loss: 0.007345033343881369\n",
      "Epoch 144, Loss: 0.007345099933445454\n",
      "Epoch 145, Loss: 0.007345113437622786\n",
      "Epoch 146, Loss: 0.007345039397478104\n",
      "Epoch 147, Loss: 0.007345063611865044\n",
      "Epoch 148, Loss: 0.007345099002122879\n",
      "Epoch 149, Loss: 0.007345049176365137\n",
      "Epoch 150, Loss: 0.007345043122768402\n",
      "Epoch 151, Loss: 0.007345080841332674\n",
      "Epoch 152, Loss: 0.007345053367316723\n",
      "Epoch 153, Loss: 0.0073450347408652306\n",
      "Epoch 154, Loss: 0.007345064077526331\n",
      "Epoch 155, Loss: 0.007345056161284447\n",
      "Epoch 156, Loss: 0.007345033809542656\n",
      "Epoch 157, Loss: 0.007345051970332861\n",
      "Epoch 158, Loss: 0.007345053367316723\n",
      "Epoch 159, Loss: 0.007345034275203943\n",
      "Epoch 160, Loss: 0.007345043122768402\n",
      "Epoch 161, Loss: 0.007345050107687712\n",
      "Epoch 162, Loss: 0.007345035672187805\n",
      "Epoch 163, Loss: 0.007345038000494242\n",
      "Epoch 164, Loss: 0.007345046382397413\n",
      "Epoch 165, Loss: 0.00734503660351038\n",
      "Epoch 166, Loss: 0.007345035206526518\n",
      "Epoch 167, Loss: 0.0073450421914458275\n",
      "Epoch 168, Loss: 0.007345037534832954\n",
      "Epoch 169, Loss: 0.007345034275203943\n",
      "Epoch 170, Loss: 0.007345040328800678\n",
      "Epoch 171, Loss: 0.007345037069171667\n",
      "Epoch 172, Loss: 0.007345033809542656\n",
      "Epoch 173, Loss: 0.007345037534832954\n",
      "Epoch 174, Loss: 0.007345037069171667\n",
      "Epoch 175, Loss: 0.007345033343881369\n",
      "Epoch 176, Loss: 0.00734503660351038\n",
      "Epoch 177, Loss: 0.00734503660351038\n",
      "Epoch 178, Loss: 0.007345033343881369\n",
      "Epoch 179, Loss: 0.007345034275203943\n",
      "Epoch 180, Loss: 0.0073450347408652306\n",
      "Epoch 181, Loss: 0.007345033809542656\n",
      "Epoch 182, Loss: 0.007345034275203943\n",
      "Epoch 183, Loss: 0.0073450347408652306\n",
      "Epoch 184, Loss: 0.007345033343881369\n",
      "Epoch 185, Loss: 0.007345034275203943\n",
      "Epoch 186, Loss: 0.007345034275203943\n",
      "Epoch 187, Loss: 0.007345033809542656\n",
      "Epoch 188, Loss: 0.007345034275203943\n",
      "Epoch 189, Loss: 0.007345033809542656\n",
      "Epoch 190, Loss: 0.007345032878220081\n",
      "Epoch 191, Loss: 0.007345034275203943\n",
      "Epoch 192, Loss: 0.007345033809542656\n",
      "Epoch 193, Loss: 0.007345033809542656\n",
      "Epoch 194, Loss: 0.007345033343881369\n",
      "Epoch 195, Loss: 0.007345033343881369\n",
      "Epoch 196, Loss: 0.007345033343881369\n",
      "Epoch 197, Loss: 0.007345032878220081\n",
      "Epoch 198, Loss: 0.007345033343881369\n",
      "Epoch 199, Loss: 0.007345033343881369\n",
      "Epoch 200, Loss: 0.007345033343881369\n",
      "Epoch 201, Loss: 0.007345033343881369\n",
      "Epoch 202, Loss: 0.007345033809542656\n",
      "Epoch 203, Loss: 0.007345033343881369\n",
      "Epoch 204, Loss: 0.007345032878220081\n",
      "Epoch 205, Loss: 0.007345033809542656\n",
      "Epoch 206, Loss: 0.007345033343881369\n",
      "Epoch 207, Loss: 0.007345033809542656\n",
      "Epoch 208, Loss: 0.007345033343881369\n",
      "Epoch 209, Loss: 0.007345033343881369\n",
      "Epoch 210, Loss: 0.007345032878220081\n",
      "Epoch 211, Loss: 0.007345033343881369\n",
      "Epoch 212, Loss: 0.007345032878220081\n",
      "Epoch 213, Loss: 0.007345033343881369\n",
      "Epoch 214, Loss: 0.007345033343881369\n",
      "Epoch 215, Loss: 0.007345033343881369\n",
      "Epoch 216, Loss: 0.007345033809542656\n",
      "Epoch 217, Loss: 0.007345033343881369\n",
      "Epoch 218, Loss: 0.007345033343881369\n",
      "Epoch 219, Loss: 0.007345033343881369\n",
      "Epoch 220, Loss: 0.007345033343881369\n",
      "Epoch 221, Loss: 0.007345033343881369\n",
      "Epoch 222, Loss: 0.007345033809542656\n",
      "Epoch 223, Loss: 0.007345032878220081\n",
      "Epoch 224, Loss: 0.007345032878220081\n",
      "Epoch 225, Loss: 0.007345033343881369\n",
      "Epoch 226, Loss: 0.007345032878220081\n",
      "Epoch 227, Loss: 0.007345033343881369\n",
      "Epoch 228, Loss: 0.007345033343881369\n",
      "Epoch 229, Loss: 0.007345033343881369\n",
      "Epoch 230, Loss: 0.007345033343881369\n",
      "Epoch 231, Loss: 0.007345032878220081\n",
      "Epoch 232, Loss: 0.007345032878220081\n",
      "Epoch 233, Loss: 0.007345033343881369\n",
      "Epoch 234, Loss: 0.007345032878220081\n",
      "Epoch 235, Loss: 0.007345033809542656\n",
      "Epoch 236, Loss: 0.007345033343881369\n",
      "Epoch 237, Loss: 0.007345032878220081\n",
      "Epoch 238, Loss: 0.007345033343881369\n",
      "Epoch 239, Loss: 0.007345033343881369\n",
      "Epoch 240, Loss: 0.007345032878220081\n",
      "Epoch 241, Loss: 0.007345033343881369\n",
      "Epoch 242, Loss: 0.007345033343881369\n",
      "Epoch 243, Loss: 0.007345032878220081\n",
      "Epoch 244, Loss: 0.007345033343881369\n",
      "Epoch 245, Loss: 0.007345033343881369\n",
      "Epoch 246, Loss: 0.007345032878220081\n",
      "Epoch 247, Loss: 0.007345033343881369\n",
      "Epoch 248, Loss: 0.007345032878220081\n",
      "Epoch 249, Loss: 0.007345031946897507\n",
      "Epoch 250, Loss: 0.007345032878220081\n",
      "Epoch 251, Loss: 0.007345033343881369\n",
      "Epoch 252, Loss: 0.007345033343881369\n",
      "Epoch 253, Loss: 0.007345033809542656\n",
      "Epoch 254, Loss: 0.007345032878220081\n",
      "Epoch 255, Loss: 0.007345033343881369\n",
      "Epoch 256, Loss: 0.007345032878220081\n",
      "Epoch 257, Loss: 0.007345032878220081\n",
      "Epoch 258, Loss: 0.007345032878220081\n",
      "Epoch 259, Loss: 0.007345033343881369\n",
      "Epoch 260, Loss: 0.007345033343881369\n",
      "Epoch 261, Loss: 0.007345032878220081\n",
      "Epoch 262, Loss: 0.007345032878220081\n",
      "Epoch 263, Loss: 0.007345032878220081\n",
      "Epoch 264, Loss: 0.007345033343881369\n",
      "Epoch 265, Loss: 0.007345033343881369\n",
      "Epoch 266, Loss: 0.007345032878220081\n",
      "Epoch 267, Loss: 0.007345032878220081\n",
      "Epoch 268, Loss: 0.007345033343881369\n",
      "Epoch 269, Loss: 0.007345032878220081\n",
      "Epoch 270, Loss: 0.007345032878220081\n",
      "Epoch 271, Loss: 0.007345033343881369\n",
      "Epoch 272, Loss: 0.007345033343881369\n",
      "Epoch 273, Loss: 0.007345033343881369\n",
      "Epoch 274, Loss: 0.007345032878220081\n",
      "Epoch 275, Loss: 0.007345031946897507\n",
      "Epoch 276, Loss: 0.007345032878220081\n",
      "Epoch 277, Loss: 0.007345032878220081\n",
      "Epoch 278, Loss: 0.007345032878220081\n",
      "Epoch 279, Loss: 0.007345031946897507\n",
      "Epoch 280, Loss: 0.007345032878220081\n",
      "Epoch 281, Loss: 0.007345032878220081\n",
      "Epoch 282, Loss: 0.007345033343881369\n",
      "Epoch 283, Loss: 0.007345031946897507\n",
      "Epoch 284, Loss: 0.007345031946897507\n",
      "Epoch 285, Loss: 0.007345031946897507\n",
      "Epoch 286, Loss: 0.007345032878220081\n",
      "Epoch 287, Loss: 0.007345032878220081\n",
      "Epoch 288, Loss: 0.007345032878220081\n",
      "Epoch 289, Loss: 0.007345032878220081\n",
      "Epoch 290, Loss: 0.007345033343881369\n",
      "Epoch 291, Loss: 0.007345033343881369\n",
      "Epoch 292, Loss: 0.007345032878220081\n",
      "Epoch 293, Loss: 0.007345032878220081\n",
      "Epoch 294, Loss: 0.007345031946897507\n",
      "Epoch 295, Loss: 0.007345031946897507\n",
      "Epoch 296, Loss: 0.007345031946897507\n",
      "Epoch 297, Loss: 0.007345032878220081\n",
      "Epoch 298, Loss: 0.007345033343881369\n",
      "Epoch 299, Loss: 0.007345032878220081\n",
      "Epoch 300, Loss: 0.007345033809542656\n",
      "Epoch 301, Loss: 0.007345032878220081\n",
      "Epoch 302, Loss: 0.007345031946897507\n",
      "Epoch 303, Loss: 0.007345033343881369\n",
      "Epoch 304, Loss: 0.007345033343881369\n",
      "Epoch 305, Loss: 0.007345032878220081\n",
      "Epoch 306, Loss: 0.007345031946897507\n",
      "Epoch 307, Loss: 0.007345032878220081\n",
      "Epoch 308, Loss: 0.007345032878220081\n",
      "Epoch 309, Loss: 0.007345032878220081\n",
      "Epoch 310, Loss: 0.007345031946897507\n",
      "Epoch 311, Loss: 0.007345032878220081\n",
      "Epoch 312, Loss: 0.007345032878220081\n",
      "Epoch 313, Loss: 0.007345033343881369\n",
      "Epoch 314, Loss: 0.007345033343881369\n",
      "Epoch 315, Loss: 0.007345033343881369\n",
      "Epoch 316, Loss: 0.007345032878220081\n",
      "Epoch 317, Loss: 0.007345031946897507\n",
      "Epoch 318, Loss: 0.007345032878220081\n",
      "Epoch 319, Loss: 0.007345032878220081\n",
      "Epoch 320, Loss: 0.007345033343881369\n",
      "Epoch 321, Loss: 0.007345031481236219\n",
      "Epoch 322, Loss: 0.007345032878220081\n",
      "Epoch 323, Loss: 0.007345033343881369\n",
      "Epoch 324, Loss: 0.007345032878220081\n",
      "Epoch 325, Loss: 0.007345032878220081\n",
      "Epoch 326, Loss: 0.007345033343881369\n",
      "Epoch 327, Loss: 0.007345031481236219\n",
      "Epoch 328, Loss: 0.007345032878220081\n",
      "Epoch 329, Loss: 0.007345032878220081\n",
      "Epoch 330, Loss: 0.007345032878220081\n",
      "Epoch 331, Loss: 0.007345033343881369\n",
      "Epoch 332, Loss: 0.007345032878220081\n",
      "Epoch 333, Loss: 0.007345033343881369\n",
      "Epoch 334, Loss: 0.007345031946897507\n",
      "Epoch 335, Loss: 0.007345031946897507\n",
      "Epoch 336, Loss: 0.007345031946897507\n",
      "Epoch 337, Loss: 0.007345032878220081\n",
      "Epoch 338, Loss: 0.007345032878220081\n",
      "Epoch 339, Loss: 0.007345031946897507\n",
      "Epoch 340, Loss: 0.007345031946897507\n",
      "Epoch 341, Loss: 0.007345031946897507\n",
      "Epoch 342, Loss: 0.007345031481236219\n",
      "Epoch 343, Loss: 0.007345031946897507\n",
      "Epoch 344, Loss: 0.007345031946897507\n",
      "Epoch 345, Loss: 0.007345032878220081\n",
      "Epoch 346, Loss: 0.007345032878220081\n",
      "Epoch 347, Loss: 0.007345032878220081\n",
      "Epoch 348, Loss: 0.007345031946897507\n",
      "Epoch 349, Loss: 0.007345032878220081\n",
      "Epoch 350, Loss: 0.007345032878220081\n",
      "Epoch 351, Loss: 0.007345031946897507\n",
      "Epoch 352, Loss: 0.007345032878220081\n",
      "Epoch 353, Loss: 0.007345031946897507\n",
      "Epoch 354, Loss: 0.007345031946897507\n",
      "Epoch 355, Loss: 0.007345032878220081\n",
      "Epoch 356, Loss: 0.007345031946897507\n",
      "Epoch 357, Loss: 0.007345032878220081\n",
      "Epoch 358, Loss: 0.007345031946897507\n",
      "Epoch 359, Loss: 0.007345032878220081\n",
      "Epoch 360, Loss: 0.007345031946897507\n",
      "Epoch 361, Loss: 0.007345032878220081\n",
      "Epoch 362, Loss: 0.007345031481236219\n",
      "Epoch 363, Loss: 0.007345031946897507\n",
      "Epoch 364, Loss: 0.007345031481236219\n",
      "Epoch 365, Loss: 0.007345031946897507\n",
      "Epoch 366, Loss: 0.007345031946897507\n",
      "Epoch 367, Loss: 0.007345032878220081\n",
      "Epoch 368, Loss: 0.007345031946897507\n",
      "Epoch 369, Loss: 0.007345031946897507\n",
      "Epoch 370, Loss: 0.007345031946897507\n",
      "Epoch 371, Loss: 0.007345031946897507\n",
      "Epoch 372, Loss: 0.007345031946897507\n",
      "Epoch 373, Loss: 0.007345031946897507\n",
      "Epoch 374, Loss: 0.007345032878220081\n",
      "Epoch 375, Loss: 0.007345033343881369\n",
      "Epoch 376, Loss: 0.007345031946897507\n",
      "Epoch 377, Loss: 0.007345032878220081\n",
      "Epoch 378, Loss: 0.007345031946897507\n",
      "Epoch 379, Loss: 0.007345031946897507\n",
      "Epoch 380, Loss: 0.007345031946897507\n",
      "Epoch 381, Loss: 0.007345031946897507\n",
      "Epoch 382, Loss: 0.007345033343881369\n",
      "Epoch 383, Loss: 0.007345031946897507\n",
      "Epoch 384, Loss: 0.007345032878220081\n",
      "Epoch 385, Loss: 0.007345031946897507\n",
      "Epoch 386, Loss: 0.007345033343881369\n",
      "Epoch 387, Loss: 0.007345032878220081\n",
      "Epoch 388, Loss: 0.007345031946897507\n",
      "Epoch 389, Loss: 0.007345031946897507\n",
      "Epoch 390, Loss: 0.007345031946897507\n",
      "Epoch 391, Loss: 0.007345031946897507\n",
      "Epoch 392, Loss: 0.007345031946897507\n",
      "Epoch 393, Loss: 0.007345033343881369\n",
      "Epoch 394, Loss: 0.007345031946897507\n",
      "Epoch 395, Loss: 0.007345031946897507\n",
      "Epoch 396, Loss: 0.007345032878220081\n",
      "Epoch 397, Loss: 0.007345031946897507\n",
      "Epoch 398, Loss: 0.007345031946897507\n",
      "Epoch 399, Loss: 0.007345031946897507\n",
      "Epoch 400, Loss: 0.007345031946897507\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming your inputs_train and outputs_train are numpy arrays and need to be converted to PyTorch tensors\n",
    "inputs_train = torch.tensor(inputs_train, dtype=torch.float32)\n",
    "outputs_train = torch.tensor(outputs_train, dtype=torch.float32)\n",
    "\n",
    "# Define the model\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(585, 256)  \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)  \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 64)   \n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleMLP()\n",
    "print(next(model.parameters()).dtype)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "model.train()  # Set the model to training mode\n",
    "for epoch in range(400):  # Number of epochs\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    outputs_model = model(inputs_train)\n",
    "    loss = criterion(outputs_model, outputs_train)\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update the weights\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Evaluate on training data\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m train_actuals, train_predictions, train_r2 \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Evaluate on testing data\u001b[39;00m\n\u001b[1;32m     43\u001b[0m test_actuals, test_predictions, test_r2 \u001b[38;5;241m=\u001b[39m evaluate_model(test_loader, model)\n",
      "Cell \u001b[0;32mIn[56], line 21\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(data_loader, model, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m true_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(true_values, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     19\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(predictions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 21\u001b[0m r2_scores \u001b[38;5;241m=\u001b[39m [r2_score(true_values[:, i], predictions[:, i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m true_values, predictions, r2_scores\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Function to perform predictions and calculate metrics\n",
    "def evaluate_model(data_loader, model, device='cpu'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    true_values = []\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():  # Temporarily set all the requires_grad flags to false\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            true_values.append(targets.cpu())\n",
    "            predictions.append(outputs.cpu())\n",
    "\n",
    "    true_values = torch.cat(true_values, dim=0).numpy()\n",
    "    predictions = torch.cat(predictions, dim=0).numpy()\n",
    "    \n",
    "    r2_scores = [r2_score(true_values[:, i], predictions[:, i]) for i in range(true_values.shape[1])]\n",
    "    \n",
    "    return true_values, predictions, r2_scores\n",
    "\n",
    "# Assuming inputs_train, outputs_train, inputs_test, outputs_test are defined and loaded\n",
    "# Creating data loaders for batch processing (optional, beneficial if data is large)\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "inputs_test = torch.tensor(inputs_test, dtype=torch.float32)\n",
    "outputs_test = torch.tensor(outputs_test, dtype=torch.float32)\n",
    "# Convert data to datasets\n",
    "train_dataset = TensorDataset(inputs_train, outputs_train)\n",
    "test_dataset = TensorDataset(inputs_test, outputs_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "# Evaluate on training data\n",
    "train_actuals, train_predictions, train_r2 = evaluate_model(train_loader, model)\n",
    "\n",
    "# Evaluate on testing data\n",
    "test_actuals, test_predictions, test_r2 = evaluate_model(test_loader, model)\n",
    "\n",
    "# Print R^2 scores\n",
    "print(\"Training R^2 scores for each output:\", train_r2)\n",
    "print(\"Testing R^2 scores for each output:\", test_r2)\n",
    "\n",
    "# Plotting function\n",
    "def plot_results(actuals, predictions, title):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(actuals.shape[1]):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        plt.plot(actuals[:, i], 'b-', label='Actual')\n",
    "        plt.plot(predictions[:, i], 'r--', label='Predicted')\n",
    "        plt.title(f'{title} - Output {i+1}')\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Output Value')\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot results\n",
    "plot_results(train_actuals, train_predictions, \"Training Data Predictions vs Actuals\")\n",
    "plot_results(test_actuals, test_predictions, \"Testing Data Predictions vs Actuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actual' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[43mactual\u001b[49m[:, \u001b[38;5;241m0\u001b[39m], predicted[:, \u001b[38;5;241m0\u001b[39m], alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted vs. Actual Values\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual Values\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'actual' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(actual[:, 0], predicted[:, 0], alpha=0.5)\n",
    "plt.title('Predicted vs. Actual Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=3)  # Diagonal line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#inputs_test = torch.tensor(inputs_test.astype('float32'))\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#outputs_test = torch.tensor(outputs_test.astype('float32'))\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#inputs_test = torch.tensor(inputs_test.astype('float32'))\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#outputs_test = torch.tensor(outputs_test.astype('float32'))\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1112\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1090\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/scratch/project_462000451/daniel/daniel_sprint/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2185\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2182\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2184\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2185\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2187\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/project_462000451/daniel/daniel_sprint/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2254\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[1;32m   2252\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[0;32m-> 2254\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2255\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   2257\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/cray/pe/python/3.11.7/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/cray/pe/python/3.11.7/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#inputs_test = torch.tensor(inputs_test.astype('float32'))\n",
    "#outputs_test = torch.tensor(outputs_test.astype('float32'))\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():  # Temporarily set all the requires_grad flags to false\n",
    "    predictions = model(inputs_test)\n",
    "\n",
    "# Calculate the Mean Squared Error and Mean Absolute Error\n",
    "mse = torch.nn.functional.mse_loss(predictions, outputs_test)\n",
    "mae = torch.nn.functional.l1_loss(predictions, outputs_test)\n",
    "print(f\"Mean Squared Error: {mse.item()}\")\n",
    "print(f\"Mean Absolute Error: {mae.item()}\")\n",
    "\n",
    "# Convert predictions and actuals to numpy for plotting\n",
    "predicted = predictions.numpy()\n",
    "actual = outputs_test.numpy()\n",
    "\n",
    "# Plotting the first output dimension as an example\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(actual[:, 0], predicted[:, 0])\n",
    "#plt.plot(actual[:, 0], 'b-', label='Actual')\n",
    "#plt.plot(predicted[:, 0], 'r--', label='Predicted')\n",
    "plt.title('Comparison of Actual and Predicted Values')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Output Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 14:59:42.220757: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740488382.896031  175159 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740488383.098530  175159 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-25 14:59:45.884576: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/scratch/project_462000451/daniel/daniel_sprint/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-02-25 15:00:28.166647: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=584, activation='relu'),  # Input layer with 584 input features and ReLU activation\n",
    "    Dense(64, activation='relu'),  \n",
    "    Dense(4, activation='linear') \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "              loss='mse', \n",
    "              metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(inputs_train, outputs_train,\n",
    "                    epochs=100,  \n",
    "                    batch_size=10,  \n",
    "                    verbose=1) \n",
    "\n",
    "# To access the training loss values and metrics:\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daniel_sprint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
